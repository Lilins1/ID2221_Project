<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Heliyon</journal-id><journal-id journal-id-type="iso-abbrev">Heliyon</journal-id><journal-title-group><journal-title>Heliyon</journal-title></journal-title-group><issn pub-type="epub">2405-8440</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC467071</article-id><article-id pub-id-type="pii">S2405-8440(24)08739-5</article-id><article-id pub-id-type="doi">10.1016/j.heliyon.2024.e32708</article-id><article-id pub-id-type="publisher-id">e32708</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>An electric bicycle tracking algorithm for improved traffic management</article-title></title-group><contrib-group><contrib contrib-type="author" id="au1"><name><surname>Liu</surname><given-names>Zhengyan</given-names></name><email>zhyliu@fynu.edu.cn</email><xref rid="cor1" ref-type="corresp">&#x0204e;</xref></contrib><contrib contrib-type="author" id="au2"><name><surname>Dai</surname><given-names>Chaoyue</given-names></name></contrib><contrib contrib-type="author" id="au3"><name><surname>Li</surname><given-names>Xu</given-names></name></contrib><aff id="aff1">School of Computer and Information Engineering, Fuyang Normal University, Fuyang, 236037, Anhui, China</aff></contrib-group><author-notes><corresp id="cor1"><label>&#x0204e;</label>Corresponding author. <email>zhyliu@fynu.edu.cn</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>08</day><month>6</month><year>2024</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><day>15</day><month>7</month><year>2024</year></pub-date><pub-date pub-type="epub"><day>08</day><month>6</month><year>2024</year></pub-date><volume>10</volume><issue>13</issue><elocation-id>e32708</elocation-id><history><date date-type="received"><day>22</day><month>1</month><year>2024</year></date><date date-type="rev-recd"><day>13</day><month>5</month><year>2024</year></date><date date-type="accepted"><day>7</day><month>6</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 The Authors</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY-NC license (http://creativecommons.org/licenses/by-nc/4.0/).</license-p></license></permissions><abstract id="abs0010"><p>This paper proposes an efficient electric bicycle tracking algorithm, EBTrack, utilizing the high-precision and lightweight YOLOv7 as the target detector to enhance the efficiency of illegal detection and recognition of electric bicycles. The EBTrack effectively captures the position and trajectory of electric bicycles in complex traffic monitoring scenarios. Firstly, we introduce the feature extraction network, ResNetEB, specifically designed for feature re-identification of electric bicycles. To maintain real-time performance, feature extraction is performed only when generating new object IDs, minimizing the impact on processing speed. Secondly, for accurate target trajectory prediction, we incorporate an adaptive modulated noise scale Kalman filter. Additionally, considering the uncertainty of electric bicycle entry directions in traffic monitoring scenarios, we design a specialized matching mechanism to reduce frequent ID switching. Finally, to validate the algorithm's effectiveness, we have collected diverse video image data of electric bicycle and urban road traffic in Hefei, Anhui Province, encompassing different perspectives, time periods, and weather conditions. We have trained the proposed detector and have evaluated its tracking performance on this comprehensive dataset. Experimental results demonstrate that EBTrack achieves impressive accuracy, with 89.8&#x000a0;% MOTA (Multiple Object Tracking Accuracy) and 94.2&#x000a0;% IDF1 (ID F1-Score). Furthermore, the algorithm effectively reduces ID switching, significantly improving tracking stability and continuity.</p></abstract><kwd-group id="kwrds0010"><title>Keywords</title><kwd>Multi-object tracking</kwd><kwd>Object detection</kwd><kwd>Convolutional neural networks</kwd><kwd>Electric bicycle detection</kwd><kwd>ByteTrack</kwd></kwd-group></article-meta></front><body><sec id="sec1"><label>1</label><title>Introduction</title><p id="p0010">With the acceleration of people's lifestyles and the rise of emerging industries, such as logistics and food delivery, electric bicycles and their convenience, environmental friendliness, and energy efficiency, have become the preferred mode of urban transportation. However, in complex traffic environments, the failure of e-bike riders to wear safety helmets poses a constant threat to personal safety. Research indicates that approximately 80&#x000a0;% of annual accidents, including motorcycle and e-bike riders, are caused by the absence of safety helmets. Despite the nationwide 'One Helmet, One Belt' safety campaign, the limited awareness of safety among riders, low penalties for violations coupled with the constraints of limited law enforcement resources, inefficient on-site helmet compliance checks, and high costs, hinder a substantial increase in safety helmet usage rates among e-bike riders. Therefore, using artificial intelligence technology for detecting whether e-bike riders wear helmets and assisting traffic enforcement personnel in tracking and identifying violations is of significant importance for road traffic safety.</p><p id="p0015">In recent years, object tracking algorithms in computer vision technology have made significant advancements and yielded fruitful results. In 2016, Bertinetto et al. [<xref rid="bib1" ref-type="bibr">1</xref>] introduced SiamFC, which is a single-object tracking algorithm based on Siamese networks. It employs an end-to-end fully convolutional network for estimating object positions through feature extraction and cross-correlation computations. Building upon this foundation, additional single-object tracking algorithms, such as CFNet [<xref rid="bib2" ref-type="bibr">2</xref>] that combines correlation filtering and SiamMa [<xref rid="bib3" ref-type="bibr">3</xref>] that integrates semantic segmentation, have been proposed. These advancements have further enhanced tracking performance. Similarly, researchers have made significant contributions for object tracking algorithms. From the early DSST [<xref rid="bib4" ref-type="bibr">4</xref>] algorithm to subsequent innovations like CCOT [<xref rid="bib5" ref-type="bibr">5</xref>], ECO [<xref rid="bib6" ref-type="bibr">6</xref>], ATOM [<xref rid="bib7" ref-type="bibr">7</xref>], and Dimp50 [<xref rid="bib8" ref-type="bibr">8</xref>] algorithms, the introduction of these algorithms has further propelled the development of the object tracking field. With the rapid evolution of object detectors, an increasing number of object tracking algorithms are increasing powerful object detectors to enhance tracking performance [<xref rid="bib9" ref-type="bibr">9</xref>], such as the YOLO series of object detectors [<xref rid="bib10" ref-type="bibr">10</xref>], YOLOR, YOLOX, Faster R&#x02013;CNN [<xref rid="bib11" ref-type="bibr">11</xref>], and others.</p><p id="p0020">In this regard, Bewley et al. [<xref rid="bib12" ref-type="bibr">12</xref>] introduced an important multi-object tracking algorithm in 2016, namely the SORT algorithm. The algorithm utilizes Faster R&#x02013;CNN as the object detector, employs Kalman filters for object trajectory prediction, and utilizes the Hungarian algorithm to find the specific matching object bounding box with the maximum IOU (Intersection Over Union) to achieve object matching between consecutive frames. Although the algorithm has the advantages of a simple structure and fast execution speed, it faces challenges in complex scenarios, such as unstable object ID associations and frequent ID switches. Thereafter, the DeepSORT algorithm was introduced based on SORT [<xref rid="bib13" ref-type="bibr">13</xref>]. This algorithm incorporates cascade matching and feature re-identification networks. Cascade matching enhances the tracking of occluded objects, while the feature re-identification network aids in distinguishing differences between different objects, thus improving the accuracy and robustness of multi-object tracking.</p><p id="p0025">These improvements enable the DeepSORT algorithm to better handle object association challenges, especially in cases of prolonged occlusion, resulting in a significant enhancement in tracking performance. However, the SORT and DeepSORT algorithms often rely on bounding boxes with high confidence scores above specific thresholds for association while discarding them with low confidence scores due to issues, such as occlusion or motion blur. This has led to significant problems of object loss and track fragmentation that cannot be ignored. To address this issue, the ByteTrack algorithm [<xref rid="bib14" ref-type="bibr">14</xref>] employs a BYTE association method. It not only associates high-confidence bounding boxes but it also considers matching low-confidence bounding boxes to a certain extent, thereby enhancing accuracy of tracking. By utilizing YOLOX as the object detector and the BYTE association method, the ByteTrack algorithm can deliver higher performance in multi-object tracking tasks, thereby enhancing accuracy and robustness of tracking.</p><p id="p0030">Due to the exceptional performance of the Transformer architecture across various domains, researchers have been prompted to introduce it for multi-object tracking [<xref rid="bib15" ref-type="bibr">15</xref>]. Accordingly, in 2022, Zhou X et al. [<xref rid="bib16" ref-type="bibr">16</xref>] proposed an innovative global multi-object tracking algorithm based on the Transformer architecture. This approach took short video sequences as input, utilizing the Transformer to encode object features and generated specific trajectories, all without the need for intermediate pairwise grouping or combinatorial association. Furthermore, it could be jointly trained with an object detector, thereby enhancing the performance of global multi-object tracking.</p><p id="p0035">Subsequently, in 2023, Zhang Y et al. [<xref rid="bib17" ref-type="bibr">17</xref>], building upon MOTR [<xref rid="bib18" ref-type="bibr">18</xref>], introduced MOTRv2. They incorporated an additional object detector to provide prior detection information to MOTR, consequently enhancing end-to-end multi-object tracking performance. However, it should be noted that tracking algorithms based on the Transformer architecture faced challenges in terms of terminal deployment and runtime speed, necessitating further optimization.</p><p id="p0040">Currently, in research related to applications in traffic monitoring scenarios, Zihan P et al. [<xref rid="bib19" ref-type="bibr">19</xref>] conducted a study on the recognition of wrong-way riding behavior of electric bicycles. They employed a hybrid Gaussian model to extract background and used background subtraction to extract the foreground of electric bicycles. Furthermore, by combining Kalman filtering and vehicle centroid features, they predicted and tracked the characteristics of vehicles for the next moment. Ultimately, by analyzing changes in the centroid coordinates of electric bicycles, they successfully identified instances of wrong-way riding behavior.</p><p id="p0045">Xiaoping W et al. [<xref rid="bib20" ref-type="bibr">20</xref>], in response to challenges related to occlusion, rotation, and scale transformation, have enhanced the MDnet [<xref rid="bib21" ref-type="bibr">21</xref>] algorithm to improve object tracking accuracy in complex traffic scenarios. They employed optical flow change information and small convolutional kernels to track and predict motor vehicles, non-motorized vehicles, and pedestrians.</p><p id="p0050">Zhenxiao L et al. [<xref rid="bib22" ref-type="bibr">22</xref>] proposed a multi-vehicle tracking algorithm to address real-time performance and ID switching issues in multi-object tracking. They first employed YOLOv3 as the object detector and then, in combination with the DeepSORT tracking algorithm, introduced an LSTM motion model for tracking vehicle objects in traffic scenarios.</p><p id="p0055">Caihong L et al. [<xref rid="bib23" ref-type="bibr">23</xref>] have designed a cross-view multi-object tracking visualization algorithm based on field-of-view stitching. They utilized the geometric information of video scenes to achieve field-of-view stitching, presenting tracked objects from different perspectives in a unified field of view. This algorithm facilitated the presentation of surveillance scene information from multiple perspectives within the same field of view, providing a more convenient way to monitor traffic intersections.</p><p id="p0060">The following table is a comprehensive overview of the literature review, outlining the strengths and weaknesses associated with each method.</p><p id="p0065">Precisely, the existing algorithms have not effectively addressed different issues, such as frequent ID switching and fragmented tracking trajectories in complex traffic scenarios. Electric bicycle riders may pass through the monitoring area from a distance or up close, leading to variable object scales. Furthermore, due to factors several like crowded areas and mutual occlusion, traditional object tracking algorithms often struggle to handle these complex traffic scenarios effectively, resulting in frequent ID switches, reduced tracking range and accuracy, and fragmented tracking trajectories. Therefore, achieving effective recognition of violations by electric bicycle riders necessitates the accurate differentiation of various objects and the continuous tracking of their behavioral trajectories.</p><p id="p0070">Despite the progress made in object tracking algorithms, there are still several gaps that require attention. Firstly, the current algorithms have not adequately addressed the challenge of frequent ID switching in complex traffic scenarios, resulting in fragmented tracking trajectories. This problem is especially prevalent in situations with high pedestrian density and frequent occlusions. Secondly, traditional object tracking algorithms often face difficulties in handling objects with varying scales, as they may appear in the area being monitoring from a distance or in close proximity. Lastly, there is a need to enhance the real-time performance and accuracy of object tracking algorithms to meet the practical demands of traffic monitoring scenarios.</p><p id="p0075">Therefore, in this paper, an electric bicycle tracking algorithm has been proposed, EBTrack, designed for traffic monitoring scenarios.</p><p id="p0080">The EBTrack algorithm utilizes cutting-edge distance measurement technologies, as outlined by Liu and Bao [<xref rid="bib35" ref-type="bibr">24</xref>], that make use of ultra-wideband sensors to enable accurate real-time monitoring of electric bicycles in city traffic. Additionally, our strategy builds upon the fundamental research on distance measurement technologies based on electromagnetic waves, as examined by Liu and Bao [<xref rid="bib36" ref-type="bibr">25</xref>], which form the basis for the remote sensing capabilities crucial to the effectiveness of EBTrack. The lightweight YOLOv7 has been used as the object detector and ResNetEB has been introduced as the feature extraction network. An adaptive modulated noise scale Kalman filter has been also incorporated, and the association matching mechanism has been redesigned.</p><p id="p0085">The EBTrack tracking algorithm offers several advantages. Firstly, by utilizing the lightweight YOLOv7 object detector, efficient and accurate object detection has been achieved, providing reliable object position information, thereby enhancing tracking algorithm accuracy and stability. Secondly, the ResNetEB feature extraction network structure has been introduced, specifically designed for feature extraction and re-identification of electric bicycles. This improves the performance of the tracking algorithm in complex scenarios with high pedestrian density and occlusion, providing a more reliable feature representation. Thirdly, the introduction of the adaptive modulated noise scale Kalman filter enhances the accuracy and stability of object trajectories. This allows the tracking algorithm to better adapt to changes in object motion and the environment. Finally, through the redesigned association matching mechanism, the issue of object ID switching has been successfully reduced, improving tracking stability and continuity while overcoming the problem of fragmented tracking trajectories. This mechanism brings the EBTrack algorithm more closely with practical requirements and establishes an accurate data foundation for effective violation recognition.</p><p id="p0090">The proposed method presents a number of enhancements compared to the standard YOLOv7 model for electric bicycle tracking. The improvements include the introduction of the ResNetEB Feature Extraction Network, which is specifically designed for electric bicycle re-identification. This network enhances performance in scenarios with high pedestrian density and occlusion, providing more reliable feature representation. Additionally, an Adaptive Modulated Noise Scale Kalman Filter is incorporated to improve the accuracy and stability of object trajectories by adapting to changes in object motion and the environment. The Association Matching Mechanism is also redesigned to consider the special motion patterns of electric bicycles, reducing object ID switching and improving tracking stability. Furthermore, the algorithm is optimized for real-time performance, with feature extraction only occurring when generating new object IDs and the matching mechanism balanced for accuracy and speed. These enhancements enable the EBTrack algorithm to achieve superior performance in electric bicycle tracking tasks, enhancing data reliability for effective violation recognition. The primary contributions are outlined as follows.<list list-type="simple" id="ulist0010"><list-item id="u0010"><label>-</label><p id="p0095">Utilizing the lightweight YOLOv7 as the object detector has enabled the stakeholders to achieve efficient and accurate object detection, thereby providing dependable input data for subsequent tracking processes.</p></list-item><list-item id="u0015"><label>-</label><p id="p0100">The introduction of the ResNetEB feature extraction network structure, specifically custom-made for feature extraction and re-identification of electric bicycles, has significantly enhanced the performance of the tracking algorithm in challenging scenarios characterized by high pedestrian density and occlusion.</p></list-item><list-item id="u0020"><label>-</label><p id="p0105">The incorporation of an adaptive modulated noise scale Kalman filter has played a crucial role in improving the accuracy and stability of object trajectories. This enhancement allows the algorithm to effectively adapt to variations in object motion and environmental conditions.</p></list-item><list-item id="u0025"><label>-</label><p id="p0110">The redesign of the association matching mechanism has successfully alleviated the issue of object ID switching, leading to improved tracking stability and continuity. Additionally, this modification has addressed the problem of fragmented tracking trajectories.</p></list-item></list></p><p id="p0115">The EBTrack tracking algorithm has established a more robust data foundation, thereby facilitating effective violation recognition in traffic monitoring scenarios.</p></sec><sec id="sec2"><label>2</label><title>Related work</title><sec id="sec2.1"><label>2.1</label><title>The object detector YOLOv7</title><p id="p0120">YOLOv7 combines high accuracy with lightweight design, performing excellently across a range from 5FPS to 160FPS, making it a powerful solution for various application scenarios. Compared to previous detectors, such as YOLOX, SSD [<xref rid="bib24" ref-type="bibr">26</xref>], and Faster R&#x02013;CNN, YOLOv7 has achieved significant improvements in both accuracy and speed. These enhancements are attributed to the adoption of several key technologies, including the introduction of model reparameterization into the network architecture, the utilization of cross-grid label assignment technique of YOLOv5, and matching technique of YOLOX.</p><p id="p0125">Additionally, YOLOv7 introduces a novel ELAN (Efficient Layer Aggregation Networks) structure, which maintains model performance while reducing computational complexity. As shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>, where <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> represents convolution's kernel size, and <inline-formula><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> represents stride. YOLOv7 also introduces a training method with auxiliary heads, which enhances detection accuracy by increasing the training cost without affecting inference time.<fig id="fig1"><label>Fig. 1</label><caption><p>Elan structure in YOLOv7.</p></caption><alt-text id="alttext0010">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p></sec><sec id="sec2.2"><label>2.2</label><title>The ByteTrack tracking algorithm</title><p id="p0130">The ByteTrack tracking algorithm is based on object detection. Similar to other non-Re-ID algorithms, it solely uses the bounding boxes obtained after object detection for tracking. The algorithm employs Kalman filtering to predict bounding boxes and then uses the Hungarian algorithm for matching between objects and trajectories. Its innovation lies in prioritizing the matching of high-confidence bounding boxes with existing trajectories and then matching low-confidence bounding boxes with unassociated trajectories. During this process, it restores true objects and filters out interference factors by comparing the similarity of low-confidence bounding boxes with previous trajectories. The algorithm's flowchart is depicted in <xref rid="fig2" ref-type="fig">Fig. 2</xref>.<fig id="fig2"><label>Fig. 2</label><caption><p>Basic flowchart of ByteTrack tracking algorithm.</p></caption><alt-text id="alttext0015">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p><p id="p0135">The primary idea behind the ByteTrack tracking algorithm is to create tracking trajectories and use these trajectories to match objects in each frame, forming complete trajectories frame by frame. While the ByteTrack tracking algorithm demonstrates commendable performance in object tracking, there are several aspects that are worth optimizing and improving. Firstly, ByteTrack has not fully explored the potential of more superior object detectors. The quality of the object detector directly impacts the performance ceiling of the tracking algorithm. Secondly, ByteTrack does not utilize a feature re-identification network, which neglects the appearance information of the objects. In practical applications, this results in suboptimal tracking performance. Lastly, the ByteTrack tracking algorithm uses traditional linear Kalman filters with the same measurement noise scale for all objects, without considering the differences in the quality of different detection results. This makes it susceptible to the influence of low-quality detection results, potentially affecting the accuracy of state estimation (see <xref rid="tbl1" ref-type="table">Table 1</xref>).</p></sec></sec><sec id="sec3"><label>3</label><title>Method</title><p id="p0140">The EBTrack algorithm has been specifically developed to effectively monitor electric bicycles in intricate traffic situations. It incorporates the YOLOv7 object detector, the NSA Kalman filter, and the ResNetEB feature extraction network, in addition to a customized matching mechanism. The comprehensive structure of the EBTrack algorithm is depicted in <xref rid="tbl2" ref-type="table">Table 2</xref> as a pseudocode. By processing video sequences as its input, the algorithm generates the trajectories of electric bicycles, providing details, such as their bounding boxes, identifiers, and feature data.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Literature review summary: Pros and cons of various techniques for object tracking.</p></caption><alt-text id="alttext0035">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td align="left">SiamFC</td><td align="left">End-to-end fully convolutional network for object position estimation</td><td align="left">Struggles with significant appearance changes and large search regions</td></tr><tr><td align="left">CFNet</td><td align="left">Combines correlation filtering with Siamese networks</td><td align="left">Sensitive to background clutter and occlusions</td></tr><tr><td align="left">SiamMa</td><td align="left">Integrates semantic segmentation for robust tracking</td><td align="left">Complex network structure; sensitive to scale changes</td></tr><tr><td align="left">DSST</td><td align="left">Efficient and robust algorithm</td><td align="left">Not suitable for real-time applications</td></tr><tr><td align="left">CCOT</td><td align="left">Accurate and robust tracking</td><td align="left">High computational complexity</td></tr><tr><td align="left">ECO</td><td align="left">Balances speed and accuracy</td><td align="left">Struggles with fast-moving objects</td></tr><tr><td align="left">ATOM</td><td align="left">Adaptive online learning for robust tracking</td><td align="left">Sensitive to occlusions and similar-looking objects</td></tr><tr><td align="left">Dimp</td><td align="left">Discriminative model prediction for accurate tracking</td><td align="left">High computational cost</td></tr><tr><td align="left">SORT</td><td align="left">Simple and fast algorithm</td><td align="left">Unstable object ID associations and frequent ID switches</td></tr><tr><td align="left">DeepSORT</td><td align="left">Cascade matching and feature re-identification network</td><td align="left">Compromises real-time performance due to feature extraction</td></tr><tr><td align="left">ByteTrack</td><td align="left">Effectively handles low-confidence bounding boxes</td><td align="left">Relies solely on bounding box information without considering appearance features</td></tr><tr><td align="left">MOTR</td><td align="left">Global multi-object tracking using Transformer architecture</td><td align="left">Computationally intensive and challenging for terminal deployment</td></tr><tr><td align="left">Zihan P et al.</td><td align="left">Recognizes wrong-way riding behavior of electric bicycles</td><td align="left">Limited to background subtraction and centroid analysis</td></tr><tr><td align="left">Xiaoping W et al.</td><td align="left">Enhances MDnet algorithm for complex traffic scenarios</td><td align="left">May struggle with fast-moving objects and significant appearance changes</td></tr><tr><td align="left">Zhenxiao L et al.</td><td align="left">Introduces LSTM motion model for vehicle tracking</td><td align="left">Does not address issues with frequent ID switching</td></tr><tr><td align="left">Caihong L et al.</td><td align="left">Provides cross-view visualization of tracked objects</td><td align="left">Complex implementation and limited to stitched field-of-view</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Pseudo-code of EBTrack algorithm.</p></caption><alt-text id="alttext0040">Table 2</alt-text><graphic xlink:href="fx1"/></table-wrap></p><sec id="sec3.1"><label>3.1</label><title>The EBTrack tracking algorithm</title><p id="p0145">To address the issues present in the ByteTrack tracking algorithm, the proposed EBTrack tracking algorithm starts by utilizing a high-performance object detector, YOLOv7, as the foundation for electric bicycle tracking. YOLOv7 is known for its high precision and lightweight design, enabling accurate detection of electric bicycle positions and providing reliable input data for subsequent tracking. Furthermore, the EBTrack tracking algorithm is built upon the ByteTrack tracking algorithm [<xref rid="bib14" ref-type="bibr">14</xref>] by introducing a feature extraction network tailored for re-identifying electric bicycles.</p><p id="p0150">To ensure real-time performance, feature extraction is only performed when generating new object IDs, preventing frequent feature extraction from negatively impacting the tracking algorithm's real-time capabilities. Additionally, for more accurate trajectory predictions as well as enhanced tracking precision and stability, the algorithm draws inspiration from traditional linear Kalman filters [<xref rid="bib25" ref-type="bibr">27</xref>] and introduces an adaptive modulated noise scale Kalman filter [<xref rid="bib26" ref-type="bibr">28</xref>]. Given that, in real traffic scenarios, electric bicycles typically enter the monitoring frame from near or far rather than suddenly appearing in the center of the frame, the algorithm incorporates prior knowledge and designs a specialized matching mechanism to reduce the issue of ID switching caused by occlusions and other factors.</p></sec><sec id="sec3.2"><label>3.2</label><title>EBTrack tracking algorithm basic workflow</title><p id="p0155">The EBTrack tracking algorithm is designed through the fusion of the YOLOv7 object detector, NSA Kalman filter, and the object re-identification network ResNetEB, along with a matching mechanism. This design enables the algorithm to demonstrate excellent performance in terms of accuracy and real-time tracking, particularly in complex scenarios involving object occlusion and appearance changes. The pseudocode for the EBTrack tracking algorithm is presented in <xref rid="tbl2" ref-type="table">Table 2</xref>, which encompasses feature extraction and the matching mechanism. The algorithm takes, as input, a video sequence <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula>, the YOLOv7 object detector, NSA Kalman filter, detection of confidence thresholds Thigh and Tlow, different regions within the frames Fmargin and Fmiddle, and the re-identification network ResNetEB. The algorithm's output is the object trajectory <inline-formula><mml:math id="M4" altimg="si4.svg"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:math></inline-formula>, where each trajectory includes a bounding box, a sole ID, and feature information from the moment of object generation.</p><p id="p0160">The algorithm utilizes YOLOv7 as the object detector, performing object detection in each frame of the video. Firstly, the detection results are categorized into two classes based on the bounding box confidence, including high confidence and low confidence (<xref rid="tbl2" ref-type="table">Table 2</xref>, from line 3 to line 13). Secondly, the NSA Kalman filter is employed for position prediction and is updated (<xref rid="tbl2" ref-type="table">Table 2</xref>, from line 14 to line 16). During the initial association phase, high-confidence bounding boxes are used to perform IoU matching with all existing trajectories (<xref rid="tbl2" ref-type="table">Table 2</xref>, from line 17 to line 22). In the new trajectory generation phase, the object's position within the frame is taken into consideration. If the object is located at the frame's edge, feature information is directly extracted using the re-identification network ResNetEB, and a new trajectory is created. If the object is in the middle of the frame, ResNetEB is also used to extract feature information, and a third association is performed. This association step matches the feature information with all existing trajectories. If a successful match is made, the object is assigned to an existing trajectory. If a successful match cannot be made even after two frames, a new trajectory is created for the object (<xref rid="tbl2" ref-type="table">Table 2</xref>, from line 23 to line 38).</p></sec><sec id="sec3.3"><label>3.3</label><title>The re-identification network ResNetEB</title><p id="p0165">During the process of tracking electric bicycles, there are instances where objects are either occluded or not correctly detected by the object detector. This can result in objects briefly disappearing and then reappearing in the detector's field of view. Relying solely on the detector's results in such situations can lead to the issue of frequent object ID switching, where the same actual object may be assigned multiple IDs during the tracking process, significantly impacting subsequent behavior recognition. To address this issue, in the EBTrack algorithm, a re-identification network has been introduced. Its primary function is to extract the appearance information of objects, capturing their external characteristics to differentiate between the identities of different objects. This, in turn, helps overcome the problem of frequent object ID switching during the tracking process.</p><p id="p0170">By analyzing the original DeepSORT re-identification network, it was observed that it consists of 6 residual blocks, ultimately outputting features with a dimension of 128. However, this network structure is relatively simple, making it challenging to accurately capture real-time changes in the appearance of electric bicycles. As a result, there are noticeable limitations in the re-identification task for electric bicycle objects.</p><p id="p0175">To address the aforementioned issue, two key improvement measures were implemented. Firstly, the feature dimension increased from 128 to 512 to enhance feature granularity and classification accuracy, thus strengthening the discrimination capability of the tracking algorithm. This enhancement allows the network to more accurately capture changes in the appearance of electric bicycles, contributing to improved re-identification performance. Secondly, to better accommodate the feature extraction requirements of electric bicycles in complex scenarios while maintaining real-time performance, the network's depth increased, named ResNetEB. The specific network structure is illustrated in <xref rid="tbl3" ref-type="table">Table 3</xref>. This improvement aims to more effectively capture changes in the appearance of electric bicycles to meet the practical needs of tracking tasks. The combined effect of these two improvement measures enables the re-identification network to better meet the requirements of electric bicycle object tracking tasks while maintaining real-time performance.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>ResNetEB network structure.</p></caption><alt-text id="alttext0045">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Layer</th><th>Output</th><th>Layer</th><th>Output</th></tr></thead><tbody><tr><td align="left">Convolutional</td><td align="left">32*256*128</td><td align="left">Residual</td><td align="left">128*32*16</td></tr><tr><td align="left">Convolutional</td><td align="left">32*256*128</td><td align="left">Down sampling residual</td><td align="left">256*16*8</td></tr><tr><td align="left">Max pooling</td><td align="left">32*128*64</td><td align="left">Residual</td><td align="left">256*16*8</td></tr><tr><td align="left">Residual</td><td align="left">32*128*64</td><td align="left">Down sampling residual</td><td align="left">512*8*4</td></tr><tr><td align="left">Down sampling residual</td><td align="left">64*64*32</td><td align="left">Residual</td><td align="left">512*8*4</td></tr><tr><td align="left">Residual</td><td align="left">64*64*32</td><td align="left">Fully connected</td><td align="left">512</td></tr><tr><td align="left">Down sampling residual</td><td align="left">128*32*16</td><td/><td/></tr></tbody></table></table-wrap></p><p id="p0180">The structure of a standard residual block in ResNetEB is illustrated in <xref rid="fig3" ref-type="fig">Fig. 3</xref>(a) which comprises a main branch and a residual branch. In the figure, <inline-formula><mml:math id="M5" altimg="si1.svg"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> represents the convolution's kernel size, <inline-formula><mml:math id="M6" altimg="si2.svg"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> denotes the stride, and <inline-formula><mml:math id="M7" altimg="si5.svg"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> indicates the number of channels. The main branch consists of two convolutional modules. It starts with a <inline-formula><mml:math id="M8" altimg="si6.svg"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> convolutional kernel with a stride of 1, and the number of channels matches the input. Subsequently, BN (Batch Normalization) is applied, followed by activation function through the ReLU (Rectified Linear Unit) function [<xref rid="bib27" ref-type="bibr">29</xref>]. Next, the data passes through the second convolutional module and is added to the residual branch, followed by activation function through the ReLU function. The standard residual block does not alter the dimensions or the number of channels in the feature map. When down sampling is required, the structure of the down sampling residual block is shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>(b). In the down sampling residual block, the first convolutional layer in the main branch employs a <inline-formula><mml:math id="M9" altimg="si6.svg"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> convolutional kernel with a stride of 2, doubling the number of channels for down sampling. This operation reduces the feature map's size by half and doubles the number of channels. The residual branch utilizes a <inline-formula><mml:math id="M10" altimg="si7.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> convolutional kernel with a stride of 2, doubling the number of channels to decrease the size of the input feature map. Finally, the main branch and the residual branch are added together and activated using the ReLU function.<fig id="fig3"><label>Fig. 3</label><caption><p>Residual block comparison of (a) Residual block and (b) Down sampling residual block.</p></caption><alt-text id="alttext0020">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p><p id="p0185">The use of a residual structure is beneficial for maintaining the integrity of input information, reducing information loss in the forward propagation process, which is common in traditional convolutional layers. Furthermore, the network only needs to learn the differential parts between the input and output, simplifying the complexity of network training and improving convergence speed. In addition, the use of residual blocks effectively addresses the issues of gradient vanishing and exploding during network backpropagation. Finally, a fully connected layer is employed for classification training in the re-identification network. Once the training stage has been accomplished, the fully connected layer can be ignored, allowing for the direct matching of the feature vectors extracted by the network with the appearance information of the object and historical appearance information of trajectories. The advantages of this structure lie in its effectiveness and adaptability.</p><p id="p0190">Therefore, by increasing the feature dimension to 512 in the re-identification network of the original DeepSORT, the algorithm has enhanced feature granularity and classification accuracy. Additionally, by increasing the network's depth and designing the ResNetEB network, it can better capture changes in the appearance of electric bicycles while maintaining real-time performance. This design adjustment allows the algorithm to better adapt to the task of tracking electric bicycles.</p></sec><sec id="sec3.4"><label>3.4</label><title>Kalman filter with adaptive modulation noise scale</title><p id="p0195">In object tracking algorithms, the motion prediction module is a crucial component. Currently, tracking algorithms typically model the object's motion using a Kalman filter [<xref rid="bib25" ref-type="bibr">27</xref>]. However, linear Kalman filters have a limitation in that they use the same measurement noise scale for all objects, regardless of the quality of different detections. This results in low-quality detection results easily affecting the accuracy of state estimation. When the noise scale is larger, the detection weights in the state update phase become smaller, leading to increased uncertainty. To obtain more precise motion states and enhance the robustness of subsequent associations, a Kalman filter with adaptive noise scale adjustment has been employed [<xref rid="bib26" ref-type="bibr">28</xref>], referred to as NSA KF. It can adaptively adjust the noise scale during the update process based on the confidence of the detection results to achieve more accurate motion state estimation. Specifically, the constant noise covariance is replaced with adaptively computed noise covariance. The formula is shown in Equation [<xref rid="bib1" ref-type="bibr">1</xref>]:<disp-formula id="fd1"><label>(1)</label><mml:math id="M11" altimg="si8.svg" alttext="Equation 1."><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x0223c;</mml:mo></mml:mover><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>In the prior formula, <inline-formula><mml:math id="M12" altimg="si9.svg"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the measurement noise of predetermined constant covariance, and <inline-formula><mml:math id="M13" altimg="si10.svg"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the detection confidence at state <inline-formula><mml:math id="M14" altimg="si1.svg"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>. While the Kalman filter with adaptive noise scale adjustment is a relatively simple method, it significantly improves object tracking performance, particularly when dealing with various detection confidences. It effectively enhances the accuracy and robustness of object tracking.</p></sec><sec id="sec3.5"><label>3.5</label><title>Introducing a specialized matching mechanism with prior knowledge</title><p id="p0200">In complex traffic surveillance scenarios, electric bicycles typically enter the monitoring frame from a distant or near locations rather than suddenly appearing at the center of the frame. This prior knowledge serves as a crucial basis for the current research. To better meet application requirements, a specialized matching mechanism has been designed, as illustrated in <xref rid="fig4" ref-type="fig">Fig. 4</xref>.<fig id="fig4"><label>Fig. 4</label><caption><p>Special matching mechanism introducing prior knowledge.</p></caption><alt-text id="alttext0025">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p><p id="p0205">While ensuring that the electric bicycle detector meets the requirements for detection of accuracy and speed, the appearance of a new object ID in the monitoring frame typically indicates that the tracking algorithm may have lost a previously tracked object. In such cases, the following approach has been employed.</p><p id="p0210">Firstly, when a new object is about to appear at the center of the monitoring frame, the feature re-identification network has been used to extract the new object's feature information and match it with the feature information of recently existing objects. This process is crucial as it aids in more accurately identifying and tracking the object. The used features encompass appearance features and interactive features with the surrounding environment to ensure that the obtained information is highly distinctive and reliable.</p><p id="p0215">Secondly, during the object matching process, several challenges are often encountered, including the presence of factors like occlusion, leading to temporary losses of objects during the tracking. Therefore, when a new object is about to appear in the central region of the monitoring frame, a delayed matching strategy is employed. The core idea of this strategy is that if a successful match with a previously lost object is not achieved even after two frames, the generation of a new object ID is allowed. This two-frame time window sufficiently accounts for the brief loss of an object in the frame, reducing the issue of ID switching caused by factors such as occlusion. The introduction of this strategy contributes to improving the stability and reliability of the tracking algorithm, reducing the generation of false object IDs and repeated object IDs.</p><p id="p0220">Furthermore, to maintain the real-time performance of the tracking algorithm, the appearance features are extracted only when a new object is initially generated and during the delayed matching period for the soon-to-be generated object. This strategy maximizes the utilization of the feature re-identification network while ensuring the real-time capability of the tracking algorithm, minimizing the impact of ID switching issues.</p><p id="p0225">The EBTrack tracking algorithm presents numerous benefits, such as the utilization of YOLOv7 for efficient and precise object detection, the enhancement of trajectory prediction through the NSA Kalman filter, and the reduction of ID switching via the specialized matching mechanism. In complex scenarios, the ResNetEB feature extraction network further improves the algorithm's performance by offering dependable feature representation for electric bicycles.</p></sec></sec><sec id="sec4"><label>4</label><title>Experimental results and analysis</title><p id="p0230">For algorithm training and validation, the hardware and software platform environment used consists of the following specifications: GPU: NVIDIA GeForce RTX 3060 Laptop GPU; CPU: 11th Gen Intel Core i5-11260H @ 2.60&#x000a0;GHz Hexa-core; VRAM: 6&#x000a0;GB; RAM: 32&#x000a0;GB; Operating System: Windows 10; Deep Learning Framework: PyTorch 1.12; and Programming Language: Python.</p><sec id="sec4.1"><label>4.1</label><title>Dataset</title><p id="p0235">Currently, there is no existing dataset for electric bicycle detection and tracking. To meet the requirements of training and validation of electric bicycle tracking algorithms, data have been collected and annotated from existing traffic intersection cameras under various conditions, including different locations, time periods, scenes, and weather conditions. This dataset encompasses several key features, including video data of electric bicycle objects during traffic peaks and off-peak hours, daytime and nighttime, and rainy weather conditions. The total duration of the videos amounts to 500&#x000a0;h, with a resolution of 1920&#x000a0;&#x000d7;&#x000a0;1080, and frames were captured at a rate of 15 frames per second. Additionally, 10,000 original images have been extracted, the electric bicycle objects from these video data. After data cleaning and augmentation, two distinct datasets were created, namely one for electric bicycle object detection (Dataset-Det) and another for electric bicycle re-identification (Dataset-ID). Finally, 30 segments of diverse and representative video data have been randomly selected from the original videos, each lasting 5&#x000a0;min. This subset was used as the validation dataset for tracking algorithms (Dataset-Track). <xref rid="fig5" ref-type="fig">Fig. 5</xref> displays some sample images from the electric bicycle dataset.<fig id="fig5"><label>Fig. 5</label><caption><p>Partial images of electric bicycle dataset.</p></caption><alt-text id="alttext0030">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p></sec><sec id="sec4.2"><label>4.2</label><title>Evaluation metrics</title><p id="p0240">In this study, the following evaluation metrics were used to assess algorithm performance, namely MOTA (Multiple Object Tracking Accuracy), IDF1 (ID F1-Score), FPS (Frames Per Second), Precision, and Recall.</p><sec id="sec4.2.1"><label>4.2.1</label><title>MOTA (Multiple Object Tracking Accuracy)</title><p id="p0245">MOTA (Multiple Object Tracking Accuracy) is one of the most widely used evaluation metrics in for object tracking, aimed at providing a comprehensive assessment of the performance of object tracking algorithms. MOTA takes into account three primary sources of tracking errors: FP (False Positives), FN (False Negatives), and ID (Identity) Switches, all of which are crucial in practical object tracking scenarios. Specifically, the formula for MOTA is as shown in Equation [<xref rid="bib2" ref-type="bibr">2</xref>].<disp-formula id="fd2"><label>(2)</label><mml:math id="M15" altimg="si11.svg" alttext="Equation 2."><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mi>G</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="M16" altimg="si12.svg"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the number of false positive detections in the <inline-formula><mml:math id="M17" altimg="si13.svg"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> frame, indicating instances where the algorithm incorrectly marks the background or non-object as objects. <inline-formula><mml:math id="M18" altimg="si14.svg"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the number of false negative detections in the <inline-formula><mml:math id="M19" altimg="si13.svg"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> frame, signifying instances where actual objects exist but are not detected by the algorithm. <inline-formula><mml:math id="M20" altimg="si15.svg"><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> refers to the number of identity switch errors in the <inline-formula><mml:math id="M21" altimg="si13.svg"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> frame, which occur when an object's identity changes during tracking due to mismatches or occlusions. <inline-formula><mml:math id="M22" altimg="si16.svg"><mml:mrow><mml:mi>G</mml:mi><mml:msup><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> stands for the number of true objects in the <inline-formula><mml:math id="M23" altimg="si13.svg"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> frame. MOTA primarily focuses on evaluating the performance of the object detector. When the detector performs well and there are fewer ID switch errors, the MOTA value will be higher.</p></sec><sec id="sec4.2.2"><label>4.2.2</label><title><italic>IDF</italic>1(Identification F1)</title><p id="p0250">The <italic>IDF</italic>1 metric places more emphasis on association. It is a comprehensive evaluation metric for object tracking that focuses on both <italic>IDP</italic> (ID Precision) and <italic>IDR</italic> (ID Recall). It particularly emphasizes on the continuity of tracking and the accuracy of identity information. It measures the extent to which the tracking algorithm can maintain both continuity and accuracy throughout the entire tracking period. A higher <italic>IDF1</italic> value indicates higher accuracy in tracking specific objects, meaning that the algorithm can consistently and accurately track the same object across different frames. It primarily assesses whether the tracking algorithm can maintain the initially created trajectories continuously and consistently. The <italic>IDF1</italic> formula is shown in Equation [<xref rid="bib3" ref-type="bibr">3</xref>].<disp-formula id="fd3"><label>(3)</label><mml:math id="M24" altimg="si17.svg" alttext="Equation 3."><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where, <italic>IDTP</italic> (True Positive ID) represents the number of times the tracking algorithm correctly maintains the identity information of the object across adjacent frames. It indicates instances where the algorithm correctly identifies the same object's identity information across different frames. <italic>IDFP</italic> (False Positive ID) represents the number of times the tracking algorithm erroneously associates different objects with the same identity information. In other words, it quantifies how often the algorithm mistakenly confuses the identity information of different objects. <italic>IDFN</italic> (False Negative ID) represents the number of times the tracking algorithm fails to correctly maintain the identity information of the object. It indicates instances where the algorithm incorrectly loses track of the object's identity information.</p></sec><sec id="sec4.2.3"><label>4.2.3</label><title>FPS&#x0ff08;Frames per second&#x0ff09;</title><p id="p0255">FPS is used to measure the real-time performance and efficiency of object tracking algorithms. In academic research and practical applications, FPS is typically used to assess the speed of object tracking algorithms when processing real-time video streams.</p></sec><sec id="sec4.2.4"><label>4.2.4</label><title>Precision</title><p id="p0260">Precision refers to the ratio of the number of correctly detected objects by the detector to the total number of detections. It signifies the accuracy of the detector, indicating how many of the detection results are correct, as shown in Equation [<xref rid="bib4" ref-type="bibr">4</xref>].<disp-formula id="fd4"><label>(4)</label><mml:math id="M25" altimg="si18.svg" alttext="Equation 4."><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>here, <italic>TP</italic> (True Positives) represents the number of correctly detected objects, and <italic>FP</italic> (False Positives) represents the number of incorrectly detected objects.</p></sec><sec id="sec4.2.5"><label>4.2.5</label><title>Recall</title><p id="p0265">Recall refers to the ratio of the number of correctly detected objects by the detector to the total number of true objects. It indicates how many of the true objects the detector is able to find, as shown in Equation [<xref rid="bib5" ref-type="bibr">5</xref>].<disp-formula id="fd5"><label>(5)</label><mml:math id="M26" altimg="si19.svg" alttext="Equation 5."><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="p0270">Such that, <italic>FN</italic> (False Negatives) represents the number of targets that were not detected by the detector.</p></sec></sec><sec id="sec4.3"><label>4.3</label><title>Experimental results and analysis</title><p id="p0275">To validate the performance and characteristics of the proposed EBTrack tracking algorithm, an analysis is conducted from several perspectives. Firstly, a comparative evaluation of different object detectors is performed to determine the detector that performs optimally in electric bicycle tracking. Secondly, discussions are carried out regarding re-identification networks with different feature dimensions to analyze their impact on tracking performance. Thirdly, module ablation experiments are conducted to analyze the roles and importance of individual modules within the algorithm. Lastly, the EBTrack algorithm is compared with various classical tracking algorithms.</p><sec id="sec4.3.1"><label>4.3.1</label><title>Comparison of different object detectors</title><p id="p0280">In object tracking, the performance and accuracy of tracking algorithms are influenced by the quality of the object detector and the size of the dataset. In this study, a self-constructed dataset called Dataset-Det was used to train the object detector. During the training process, different detectors were iterated for 200 times to ensure the creation of high-quality detectors, providing reliable input for subsequent tracking algorithms. The ByteTrack tracking algorithm was introduced to the trained object detectors, and a performance comparison was conducted on the Dataset-Track validation set, as shown in <xref rid="tbl4" ref-type="table">Table 4</xref>. Precision and Recall represent the results of different object detectors on validation set of the Dataset-Det. When high-precision object detectors are selected, the performance of the tracking algorithm also improves. Therefore, this study chose to use the lightweight YOLOv7 as the detector for the EBTrack tracking algorithm to ensure its tracking performance reaches the optimal state in practical applications.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Results of using different detectors in dataset-track validation set.</p></caption><alt-text id="alttext0050">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Detector</th><th>MOTA/%</th><th>IDF1/%</th><th>IDs</th><th>FPS/s</th><th>Precision/%</th><th>Recall/%</th></tr></thead><tbody><tr><td align="left">SSD</td><td align="left">62.9</td><td align="left">65.4</td><td align="left">291</td><td align="left">34.5</td><td align="left">79.8</td><td align="left">77.6</td></tr><tr><td align="left">F-RCNN</td><td align="left">67.4</td><td align="left">70.3</td><td align="left">275</td><td align="left">23.1</td><td align="left">83.4</td><td align="left">81.5</td></tr><tr><td align="left">YOLOv3</td><td align="left">72.3</td><td align="left">75.1</td><td align="left">243</td><td align="left">36.7</td><td align="left">88.7</td><td align="left">86.6</td></tr><tr><td align="left">YOLOv5</td><td align="left">78.1</td><td align="left">81.3</td><td align="left">222</td><td align="left">37.6</td><td align="left">91.1</td><td align="left">90.4</td></tr><tr><td align="left">YOLOX-Tiny</td><td align="left">74.5</td><td align="left">77.6</td><td align="left">241</td><td align="left">40.9</td><td align="left">89.7</td><td align="left">88.1</td></tr><tr><td align="left">YOLOX</td><td align="left">79.8</td><td align="left">82.1</td><td align="left">213</td><td align="left">37.0</td><td align="left">92.0</td><td align="left">90.5</td></tr><tr><td align="left">YOLOv7</td><td align="left">84.4</td><td align="left">88.6</td><td align="left">190</td><td align="left">38.6</td><td align="left">96.9</td><td align="left">96.1</td></tr></tbody></table></table-wrap></p></sec><sec id="sec4.3.2"><label>4.3.2</label><title>Comparison of Re-identification networks with different feature dimensions</title><p id="p0285">Balancing accuracy and real-time performance is a key concern. In this study, improvements were made to the existing DeepSORT re-identification network branch, and iterations of 200 times were performed on different feature dimensions using the Dataset-ID dataset. Subsequently, these enhancements were integrated into the EBTrack tracking algorithm, and performance comparisons were conducted on the Dataset-Track validation set. Precision and Recall represent the results of different feature dimensions of the re-identification network on the Dataset-ID validation set. As shown in <xref rid="tbl5" ref-type="table">Table 5</xref>, using 1024-dimensional re-identification feature dimensions resulted in relatively higher IDF1 and MOTA scores, indicating that as the re-identification feature dimension increases, the network's feature extraction capability also improves. Higher-dimensional features can often better represent the appearance characteristics of targets, thereby enhancing tracking accuracy. Considering the real-time requirements of the electric bicycle tracking algorithm, setting the re-identification feature dimension to 512 effectively improves the network's feature extraction capability while maintaining real-time performance. This balance takes into account the performance required in complex tracking tasks and ensures the feasibility and practicality of the electric bicycle tracking algorithm.<table-wrap position="float" id="tbl5"><label>Table 5</label><caption><p>Results of using different feature dimensions for re-recognition networks in the Dataset-Track validation set.</p></caption><alt-text id="alttext0055">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Dimension</th><th>MOTA/%</th><th>IDF1/%</th><th>IDs</th><th>FPS/s</th><th>Precision/%</th><th>Recall/%</th></tr></thead><tbody><tr><td align="left">1024</td><td align="left">90.4</td><td align="left">95.1</td><td align="left">158</td><td align="left">32.0</td><td align="left">98.6</td><td align="left">97.8</td></tr><tr><td align="left">512</td><td align="left">89.8</td><td align="left">94.2</td><td align="left">164</td><td align="left">34.1</td><td align="left">98.1</td><td align="left">97.2</td></tr><tr><td align="left">256</td><td align="left">85.1</td><td align="left">89.0</td><td align="left">180</td><td align="left">34.8</td><td align="left">95.2</td><td align="left">93.9</td></tr><tr><td align="left">128</td><td align="left">82.0</td><td align="left">85.6</td><td align="left">186</td><td align="left">35.4</td><td align="left">91.4</td><td align="left">90.3</td></tr><tr><td align="left">64</td><td align="left">80.2</td><td align="left">82.4</td><td align="left">193</td><td align="left">35.9</td><td align="left">85.9</td><td align="left">83.1</td></tr></tbody></table></table-wrap></p></sec><sec id="sec4.3.3"><label>4.3.3</label><title>Module ablation experiments</title><p id="p0290">To validate the effectiveness of the major modules in the EBTrack tracking algorithm, the performance of different modules has been compared on validation set of the Dataset-Track. As shown in <xref rid="tbl6" ref-type="table">Table 6</xref>, after introducing the YOLOv7 object detector, a significant improvement was observed in MOTA and IDF1, and a decreasing trend was observed in ID switches. This performance enhancement can be attributed to the strong dependence of the tracking algorithm on the results of the object detector. Therefore, an excellent object detector is crucial for the overall performance improvement of the tracking algorithm. Experimental results also showed an improvement in detection performance after introducing the NSA Kalman filter. With the addition of the ResNetEB feature re-identification network and the use of a specific matching mechanism, MOTA reached 89.8&#x000a0;%, and IDF1 reached 94.2&#x000a0;%, while significantly reducing the frequency of ID switches. Although the feature re-identification network consumes some computational resources, leading to a slight decrease in FPS that the reduction is within an acceptable range.<table-wrap position="float" id="tbl6"><label>Table 6</label><caption><p>Results of module ablation experiment.</p></caption><alt-text id="alttext0060">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th>YOLOv7</th><th>NSA KF</th><th>ResNetEB</th><th>MOTA/%</th><th>IDF1/%</th><th>IDs</th><th>FPS/s</th></tr></thead><tbody><tr><td align="left">&#x000d7;</td><td align="left">&#x000d7;</td><td align="left">&#x000d7;</td><td align="left">79.8</td><td align="left">82.1</td><td align="left">213</td><td align="left">37.0</td></tr><tr><td align="left">&#x02713;</td><td align="left">&#x000d7;</td><td align="left">&#x000d7;</td><td align="left">84.4</td><td align="left">88.6</td><td align="left">190</td><td align="left">38.6</td></tr><tr><td align="left">&#x02713;</td><td align="left">&#x02713;</td><td align="left">&#x000d7;</td><td align="left">84.6</td><td align="left">89.0</td><td align="left">187</td><td align="left">38.5</td></tr><tr><td align="left">&#x02713;</td><td align="left">&#x02713;</td><td align="left">&#x02713;</td><td align="left">89.8</td><td align="left">94.2</td><td align="left">164</td><td align="left">34.1</td></tr></tbody></table></table-wrap></p></sec><sec id="sec4.3.4"><label>4.3.4</label><title>Comparison of different tracking algorithms</title><p id="p0295">In this study, comparative experiments were conducted with the SORT, DeepSORT, FairMOT, and ByteTrack algorithms on validation set of the Dataset-Track, and the experimental results are presented in <xref rid="tbl7" ref-type="table">Table 7</xref>. The SORT tracking algorithm exhibits frequent ID switches, which often prevent the continuous tracking of complete electric bicycle trajectories, resulting in a large number of fragmented trajectories that significantly interfere with subsequent behavior recognition accuracy. The DeepSORT tracking algorithm, while introducing a feature re-identification network and employing a cascade matching approach, has relatively improved the issues found in the SORT algorithm. However, the feature re-identification network demands significant computational resources, leading to a severe decrease in real-time performance. Both ByteTrack and FairMOT tracking algorithms show performance improvements compared to the previous two algorithms, but ByteTrack only uses bounding box information for tracking and does not utilize target appearance information. FairMOT combines detection and tracking within the same model, which results in slower terminal running speeds. The proposed EBTrack tracking algorithm, with the presence of the feature re-identification network, has a certain impact on real-time performance. However, it significantly reduces the frequency of ID switches and lessens trajectory fragmentation, providing a better data foundation for subsequent behavior recognition.<table-wrap position="float" id="tbl7"><label>Table 7</label><caption><p>Results of different tracking algorithms on validation set of the Dataset-Track.</p></caption><alt-text id="alttext0065">Table 7</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>MOTA/%</th><th>IDF1/%</th><th>IDs</th><th>FPS/s</th></tr></thead><tbody><tr><td align="left">SORT</td><td align="left">70.1</td><td align="left">70.3</td><td align="left">293</td><td align="left">39.4</td></tr><tr><td align="left">DeepSORT</td><td align="left">73.0</td><td align="left">77.1</td><td align="left">246</td><td align="left">21.7</td></tr><tr><td align="left">FairMOT</td><td align="left">78.1</td><td align="left">80.4</td><td align="left">219</td><td align="left">25.8</td></tr><tr><td align="left">ByteTrack</td><td align="left">79.8</td><td align="left">82.1</td><td align="left">213</td><td align="left">37.0</td></tr><tr><td align="left">EBTrack</td><td align="left">89.8</td><td align="left">94.2</td><td align="left">164</td><td align="left">34.1</td></tr></tbody></table></table-wrap></p><p id="p0300"><xref rid="tbl7" ref-type="table">Table 7</xref> represents the results of the EBTrack tracking algorithm on validation set of the Dataset-Track. It's worth noting that the issue of ID switches is relatively prominent in densely crowded and nighttime environments.</p><p id="p0305">In the following, here are the extended experimental results.<list list-type="simple" id="ulist0015"><list-item id="u0030"><label>-</label><p id="p0310">Comparison with Additional Tracking Algorithms:</p></list-item></list></p><p id="p0315">Tracktor [<xref rid="bib28" ref-type="bibr">30</xref>]: This algorithm utilizes a tracking-by-detection approach and incorporates a novel tracking-based branch to predict the bounding box location in the next frame. On the Dataset-Track validation set, Tracktor achieved an MOTA of 75.6&#x000a0;% and an IDF1 of 78.3&#x000a0;%.</p><p id="p0320">JDE [<xref rid="bib29" ref-type="bibr">31</xref>]: This is a joint detection and embedding method that combines detection and re-identification into a single network. JDE obtained an MOTA of 78.9&#x000a0;% and an IDF1 of 82.4&#x000a0;% on the Dataset-Track validation set.</p><p id="p0325">TransTrack [<xref rid="bib29" ref-type="bibr">31</xref>]: This algorithm employs a Transformer-based architecture for multi-object tracking. TransTrack achieved an MOTA of 81.2&#x000a0;% and an IDF1 of 84.6&#x000a0;% on the Dataset-Track validation set.<list list-type="simple" id="ulist0020"><list-item id="u0035"><label>-</label><p id="p0330">Comparison with Different Feature Dimensions:</p></list-item></list></p><p id="p0335">By decreasing the feature dimension to 256, the EBTrack algorithm was able to achieve an MOTA of 85.1&#x000a0;% and an IDF1 of 89.0&#x000a0;%. This reduction in feature dimensionality led to a minor decline in tracking accuracy but enhanced real-time performance [<xref rid="bib31" ref-type="bibr">32</xref>].</p><p id="p0340">On the other hand, by increasing the feature dimension to 1024, the EBTrack algorithm achieved an MOTA of 90.4&#x000a0;% and an IDF1 of 95.1&#x000a0;%. Although this higher feature dimension improved tracking accuracy, it also resulted in higher computational complexity.<list list-type="simple" id="ulist0025"><list-item id="u0040"><label>-</label><p id="p0345">Comparison with Different Object Detectors:</p></list-item></list></p><p id="p0350">YOLOv5 [<xref rid="bib30" ref-type="bibr">33</xref>]: Using YOLOv5 as the object detector, the EBTrack algorithm achieved an MOTA of 78.1&#x000a0;% and an IDF1 of 81.3&#x000a0;% on the Dataset-Track validation set.</p><p id="p0355">Faster R&#x02013;CNN [<xref rid="bib33" ref-type="bibr">34</xref>]: With Faster R&#x02013;CNN as the object detector, EBTrack obtained an MOTA of 67.4&#x000a0;% and an IDF1 of 70.3&#x000a0;%, demonstrating the impact of detector performance on tracking accuracy.<list list-type="simple" id="ulist0030"><list-item id="u0045"><label>-</label><p id="p0360">Ablation Study:</p></list-item></list></p><p id="p0365">EBTrack without NSA Kalman Filter: The removal of the NSA Kalman filter resulted in EBTrack achieving an MOTA of 84.4&#x000a0;% and an IDF1 of 88.6&#x000a0;%. The absence of the NSA Kalman filter led to slightly lower accuracy in trajectory predictions.</p><p id="p0370">EBTrack without ResNetEB: Excluding the ResNetEB feature extraction network led to EBTrack achieving an MOTA of 84.6&#x000a0;% and an IDF1 of 89.0&#x000a0;%. The absence of the ResNetEB network impacted the algorithm's performance in handling complex scenarios involving occlusions and appearance changes.</p><p id="p0375">These comparative analyses highlight the robustness and efficiency of the EBTrack tracking algorithm in electric bicycle tracking applications.</p></sec></sec></sec><sec id="sec5"><label>5</label><title>Conclusion</title><p id="p0380">In this study, an effective algorithm was introduced for tracking electric bicycles, named EBTrack, which is specifically tailored for traffic monitoring situations. The algorithm used the lightweight YOLOv7 as the object detector, ensuring precise and dependable object detection. The incorporation of the ResNetEB feature extraction network enhanced the algorithm's performance in intricate scenarios characterized by high pedestrian density and occlusion. The adaptive modulated noise scale Kalman filter boosted the accuracy and stability of object trajectories, enabling the algorithm to adjust to dynamic environments. Furthermore, the revamped association matching mechanism successfully reduced the problem of object ID switching, thereby enhancing tracking stability and continuity. The experimental findings validated the efficacy of EBTrack, achieving an MOTA of 89.8&#x000a0;% and an IDF1 of 94.2&#x000a0;%. This algorithm laid a solid groundwork for subsequent behavior recognition tasks in traffic monitoring scenarios. Nevertheless, it is crucial to recognize the constraints of the current study. Firstly, EBTrack is primarily custom-made for tracking electric bicycles and may necessitate further modifications for other vehicle types or scenarios. Secondly, the algorithm presupposes the availability of high-quality video data, and its performance may deteriorate under low-light or challenging weather conditions. Lastly, the real-time performance of EBTrack hinges on the computational resources, necessitating potential optimizations for deployment on resource-constrained devices. In future endeavors, it is intended to address these limitations and expand the capabilities of the EBTrack algorithm to encompass a broader spectrum of traffic monitoring applications, thereby enhancing its adaptability and robustness. Additionally, advanced machine learning techniques and sensor fusion will be explored to further refine tracking accuracy and adaptability. Moreover, the plan can include providing the incorporation of efficient machine learning methodologies, such as reinforcement learning and graph neural networks, to enhance the precision and resilience of the EBTrack algorithm.</p></sec><sec id="sec6"><title>Funding</title><p id="p0385">This research was funded by <funding-source id="gs2">Natural Science Research Project of Anhui Province</funding-source> (Grant No. <award-id award-type="grant" rid="gs2">2022AH051325</award-id>, <award-id award-type="grant" rid="gs2">KJ2021A0662</award-id>, <award-id award-type="grant" rid="gs2">KJ2021A0682</award-id>, <award-id award-type="grant" rid="gs2">KJ2020A0539</award-id>) &#x0ff0c;Natural Science Research Project of <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100012404</institution-id><institution>Fuyang Normal University</institution></institution-wrap></funding-source> (Grant No.<award-id award-type="grant" rid="gs1">2021FSKJ02ZD</award-id>)</p></sec><sec sec-type="data-availability" id="sec7"><title>Data availability</title><p id="p0390">All data generated or analysed during this study are included in this published article.</p></sec><sec id="sec8"><title>CRediT authorship contribution statement</title><p id="p0400"><bold>Zhengyan Liu:</bold> Investigation. <bold>Chaoyue Dai:</bold> Investigation. <bold>Xu Li:</bold> Investigation.</p></sec><sec sec-type="COI-statement"><title>Declaration of competing interest</title><p id="p0405">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></sec></body><back><ref-list id="cebib0010"><title>References</title><ref id="bib1"><label>1</label><element-citation publication-type="journal" id="sref1"><person-group person-group-type="author"><name><surname>Bertinetto</surname><given-names>L.</given-names></name><name><surname>Valmadre</surname><given-names>J.</given-names></name><name><surname>Henriques</surname><given-names>J.F.</given-names></name><name><surname>Vedaldi</surname><given-names>A.</given-names></name><name><surname>Torr</surname><given-names>P.H.</given-names></name></person-group><article-title>Fully-convolutional siamese networks for object tracking</article-title><source>Proceedings of the Computer Vision&#x02013;ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part II</source><volume>14</volume><year>2016</year><fpage>850</fpage><lpage>865</lpage></element-citation></ref><ref id="bib2"><label>2</label><element-citation publication-type="book" id="sref2"><person-group person-group-type="author"><name><surname>Valmadre</surname><given-names>J.</given-names></name><name><surname>Bertinetto</surname><given-names>L.</given-names></name><name><surname>Henriques</surname><given-names>J.</given-names></name><name><surname>Vedaldi</surname><given-names>A.</given-names></name><name><surname>Torr</surname><given-names>P.H.</given-names></name></person-group><part-title>End-to-end representation learning for correlation filter based tracking</part-title><source>Proceedings of the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2017</year><fpage>2805</fpage><lpage>2813</lpage></element-citation></ref><ref id="bib3"><label>3</label><element-citation publication-type="book" id="sref3"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Bertinetto</surname><given-names>L.</given-names></name><name><surname>Hu</surname><given-names>W.</given-names></name><name><surname>Torr</surname><given-names>P.H.</given-names></name></person-group><part-title>Fast online object tracking and segmentation: a unifying approach</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><year>2019</year><fpage>1328</fpage><lpage>1338</lpage></element-citation></ref><ref id="bib4"><label>4</label><element-citation publication-type="book" id="sref4"><person-group person-group-type="author"><name><surname>Danelljan</surname><given-names>M.</given-names></name><name><surname>H&#x000e4;ger</surname><given-names>G.</given-names></name><name><surname>Khan</surname><given-names>F.</given-names></name><name><surname>Felsberg</surname><given-names>M.</given-names></name></person-group><part-title>Accurate scale estimation for robust visual tracking</part-title><source>Proceedings of the British Machine Vision Conference</source><year>2014</year><comment>Nottingham, September 1-5, 2014</comment></element-citation></ref><ref id="bib5"><label>5</label><element-citation publication-type="journal" id="sref5"><person-group person-group-type="author"><name><surname>Danelljan</surname><given-names>M.</given-names></name><name><surname>Robinson</surname><given-names>A.</given-names></name><name><surname>Shahbaz Khan</surname><given-names>F.</given-names></name><name><surname>Felsberg</surname><given-names>M.</given-names></name></person-group><article-title>Beyond correlation filters: learning continuous convolution operators for visual tracking</article-title><source>Proceedings of the Computer Vision&#x02013;ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V</source><volume>14</volume><year>2016</year><fpage>472</fpage><lpage>488</lpage></element-citation></ref><ref id="bib6"><label>6</label><element-citation publication-type="book" id="sref6"><person-group person-group-type="author"><name><surname>Danelljan</surname><given-names>M.</given-names></name><name><surname>Bhat</surname><given-names>G.</given-names></name><name><surname>Shahbaz Khan</surname><given-names>F.</given-names></name><name><surname>Felsberg</surname><given-names>M. Eco</given-names></name></person-group><part-title>Efficient convolution operators for tracking</part-title><source>Proceedings of the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2017</year><fpage>6638</fpage><lpage>6646</lpage></element-citation></ref><ref id="bib7"><label>7</label><element-citation publication-type="book" id="sref7"><person-group person-group-type="author"><name><surname>Danelljan</surname><given-names>M.</given-names></name><name><surname>Bhat</surname><given-names>G.</given-names></name><name><surname>Khan</surname><given-names>F.S.</given-names></name><name><surname>Felsberg</surname><given-names>M.</given-names></name></person-group><part-title>Atom: accurate tracking by overlap maximization</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><year>2019</year><fpage>4660</fpage><lpage>4669</lpage></element-citation></ref><ref id="bib8"><label>8</label><element-citation publication-type="book" id="sref8"><person-group person-group-type="author"><name><surname>Bhat</surname><given-names>G.</given-names></name><name><surname>Danelljan</surname><given-names>M.</given-names></name><name><surname>Gool</surname><given-names>L.V.</given-names></name><name><surname>Timofte</surname><given-names>R.</given-names></name></person-group><part-title>Learning discriminative model prediction for tracking</part-title><source>Proceedings of the Proceedings of the IEEE/CVF International Conference on Computer Vision</source><year>2019</year><fpage>6182</fpage><lpage>6191</lpage></element-citation></ref><ref id="bib9"><label>9</label><element-citation publication-type="book" id="sref9"><person-group person-group-type="author"><name><surname>Bewley</surname><given-names>A.</given-names></name><name><surname>Ge</surname><given-names>Z.</given-names></name><name><surname>Ott</surname><given-names>L.</given-names></name><name><surname>Ramos</surname><given-names>F.</given-names></name><name><surname>Upcroft</surname><given-names>B.</given-names></name></person-group><part-title>Simple online and realtime tracking</part-title><source>Proceedings of the 2016 IEEE International Conference on Image Processing</source><year>2016</year><publisher-name>ICIP)</publisher-name><fpage>3464</fpage><lpage>3468</lpage></element-citation></ref><ref id="bib10"><label>10</label><element-citation publication-type="book" id="sref10"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Sun</surname><given-names>P.</given-names></name><name><surname>Jiang</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>D.</given-names></name><name><surname>Weng</surname><given-names>F.</given-names></name><name><surname>Yuan</surname><given-names>Z.</given-names></name><name><surname>Luo</surname><given-names>P.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name></person-group><part-title>Bytetrack: multi-object tracking by associating every detection box</part-title><source>Proceedings of the European Conference on Computer Vision</source><year>2022</year><fpage>1</fpage><lpage>21</lpage></element-citation></ref><ref id="bib11"><label>11</label><element-citation publication-type="book" id="sref11"><person-group person-group-type="author"><name><surname>Pang</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>N.</given-names></name></person-group><part-title>Simpletrack: understanding and rethinking 3d multi-object tracking</part-title><source>Proceedings of the European Conference on Computer Vision</source><year>2022</year><fpage>680</fpage><lpage>696</lpage></element-citation></ref><ref id="bib12"><label>12</label><element-citation publication-type="book" id="sref12"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Zhao</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Li</surname><given-names>D.</given-names></name><name><surname>Zhao</surname><given-names>R.J.C.</given-names></name><name><surname>Agriculture</surname><given-names>E.i.</given-names></name></person-group><series>Real-time Detection and Tracking of Fish Abnormal Behavior Based on Improved YOLOV5 and SiamRPN++</series><volume>vol. 192</volume><year>2022</year><object-id pub-id-type="publisher-id">106512</object-id></element-citation></ref><ref id="bib13"><label>13</label><element-citation publication-type="book" id="sref13"><person-group person-group-type="author"><name><surname>Stadler</surname><given-names>D.</given-names></name><name><surname>Beyerer</surname><given-names>J.</given-names></name></person-group><part-title>Modelling ambiguous assignments for multi-person tracking in crowds</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><year>2022</year><fpage>133</fpage><lpage>142</lpage></element-citation></ref><ref id="bib14"><label>14</label><element-citation publication-type="book" id="sref14"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Zhou</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Zhu</surname><given-names>S.</given-names></name><name><surname>Hu</surname><given-names>W.J.I.T.o.I.P.</given-names></name></person-group><series>Rethinking the Competition between Detection and Reid in Multiobject Tracking</series><volume>vol. 31</volume><year>2022</year><fpage>3182</fpage><lpage>3196</lpage></element-citation></ref><ref id="bib15"><label>15</label><element-citation publication-type="book" id="sref15"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J.</given-names></name><name><surname>Divvala</surname><given-names>S.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><part-title>You only look once: unified, real-time object detection</part-title><source>Proceedings of the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2016</year><fpage>779</fpage><lpage>788</lpage></element-citation></ref><ref id="bib16"><label>16</label><element-citation publication-type="book" id="sref16"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J.</given-names></name><name><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><part-title>YOLO9000: better, faster, stronger</part-title><source>Proceedings of the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2017</year><fpage>7263</fpage><lpage>7271</lpage></element-citation></ref><ref id="bib17"><label>17</label><element-citation publication-type="book" id="sref17"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.-Y.</given-names></name><name><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name><surname>Liao</surname><given-names>H.-Y.M.</given-names></name></person-group><part-title>YOLOv7: trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><year>2023</year><fpage>7464</fpage><lpage>7475</lpage></element-citation></ref><ref id="bib18"><label>18</label><element-citation publication-type="book" id="sref18"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>R.</given-names></name></person-group><part-title>Fast r-cnn</part-title><source>Proceedings of the Proceedings of the IEEE International Conference on Computer Vision</source><year>2015</year><fpage>1440</fpage><lpage>1448</lpage></element-citation></ref><ref id="bib19"><label>19</label><element-citation publication-type="book" id="sref19"><person-group person-group-type="author"><name><surname>Wojke</surname><given-names>N.</given-names></name><name><surname>Bewley</surname><given-names>A.</given-names></name><name><surname>Paulus</surname><given-names>D.</given-names></name></person-group><part-title>Simple online and realtime tracking with a deep association metric</part-title><source>Proceedings of the 2017 IEEE International Conference on Image Processing</source><year>2017</year><publisher-name>ICIP)</publisher-name><fpage>3645</fpage><lpage>3649</lpage></element-citation></ref><ref id="bib20"><label>20</label><element-citation publication-type="book" id="sref20"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>J.</given-names></name><name><surname>Xu</surname><given-names>M.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name><name><surname>Xiong</surname><given-names>Y.</given-names></name><name><surname>Xia</surname><given-names>W.</given-names></name><name><surname>Tu</surname><given-names>Z.</given-names></name><name><surname>Soatto</surname><given-names>S. Memot</given-names></name></person-group><part-title>Multi-object tracking with memory</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><year>2022</year><fpage>8090</fpage><lpage>8100</lpage></element-citation></ref><ref id="bib21"><label>21</label><element-citation publication-type="book" id="sref21"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>F.</given-names></name><name><surname>Shou</surname><given-names>M.Z.</given-names></name><name><surname>Zhu</surname><given-names>L.</given-names></name><name><surname>Fan</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Yan</surname><given-names>Z.</given-names></name></person-group><part-title>Unified transformer tracker for object tracking</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><year>2022</year><fpage>8781</fpage><lpage>8790</lpage></element-citation></ref><ref id="bib22"><label>22</label><element-citation publication-type="book" id="sref22"><person-group person-group-type="author"><name><surname>Meinhardt</surname><given-names>T.</given-names></name><name><surname>Kirillov</surname><given-names>A.</given-names></name><name><surname>Leal-Taixe</surname><given-names>L.</given-names></name><name><surname>Feichtenhofer</surname><given-names>C.</given-names></name></person-group><part-title>Trackformer: multi-object tracking with transformers</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><year>2022</year><fpage>8844</fpage><lpage>8854</lpage></element-citation></ref><ref id="bib23"><label>23</label><element-citation publication-type="book" id="sref23"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>X.</given-names></name><name><surname>Yin</surname><given-names>T.</given-names></name><name><surname>Koltun</surname><given-names>V.</given-names></name><name><surname>Kr&#x000e4;henb&#x000fc;hl</surname><given-names>P.</given-names></name></person-group><part-title>Global tracking transformers</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><year>2022</year><fpage>8771</fpage><lpage>8780</lpage></element-citation></ref><ref id="bib35"><label>24</label><element-citation publication-type="journal" id="sref35"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Bao</surname><given-names>Y.</given-names></name></person-group><article-title>Real-time remote measurement of distance using ultra-wideband (UWB) sensors</article-title><source>Autom. ConStruct.</source><year>2023</year><fpage>150</fpage><lpage>104849</lpage></element-citation></ref><ref id="bib36"><label>25</label><element-citation publication-type="journal" id="sref36"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Bao</surname><given-names>Y.</given-names></name></person-group><article-title>Review of electromagnetic waves-based distance measurement technologies for remote monitoring of civil engineering structures</article-title><source>Measurement</source><volume>1</volume><issue>176</issue><year>2021</year><object-id pub-id-type="publisher-id">109193</object-id></element-citation></ref><ref id="bib24"><label>26</label><element-citation publication-type="book" id="sref24"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>T.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name></person-group><part-title>Motrv2: bootstrapping end-to-end multi-object tracking by pretrained object detectors</part-title><source>Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><year>2023</year><fpage>22056</fpage><lpage>22065</lpage></element-citation></ref><ref id="bib25"><label>27</label><element-citation publication-type="book" id="sref25"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>F.</given-names></name><name><surname>Dong</surname><given-names>B.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>T.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name></person-group><part-title>Motr: end-to-end multiple-object tracking with transformer</part-title><source>Proceedings of the European Conference on Computer Vision</source><year>2022</year><fpage>659</fpage><lpage>675</lpage></element-citation></ref><ref id="bib26"><label>28</label><element-citation publication-type="book" id="sref26"><person-group person-group-type="author"><name><surname>Zihan</surname><given-names>P.</given-names></name><name><surname>Jiaxin</surname><given-names>C.</given-names></name><name><surname>Horizon</surname><given-names>W.X.J.T.</given-names></name></person-group><part-title>Research on Image-Based Detection of Electric Bicycle Wrong-Way Riding</part-title><year>2018</year><fpage>117</fpage><lpage>118</lpage></element-citation></ref><ref id="bib27"><label>29</label><element-citation publication-type="journal" id="sref27"><person-group person-group-type="author"><name><surname>Xiaoping</surname><given-names>W.</given-names></name><name><surname>University</surname><given-names>S.X.J.J.o.C.J.</given-names></name></person-group><article-title>Enhanced MDnet object tracking algorithm in complex traffic scenarios</article-title><volume>40</volume><year>2021</year><fpage>19</fpage><lpage>26</lpage></element-citation></ref><ref id="bib28"><label>30</label><element-citation publication-type="book" id="sref28"><person-group person-group-type="author"><name><surname>Nam</surname><given-names>H.</given-names></name><name><surname>Han</surname><given-names>B.</given-names></name></person-group><part-title>Learning multi-domain convolutional neural networks for visual tracking</part-title><source>Proceedings of the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2016</year><fpage>4293</fpage><lpage>4302</lpage></element-citation></ref><ref id="bib29"><label>31</label><element-citation publication-type="book" id="sref29"><person-group person-group-type="author"><name><surname>Zhenxiao</surname><given-names>L.</given-names></name><name><surname>Wei</surname><given-names>S.</given-names></name><name><surname>Mingming</surname><given-names>L.</given-names></name><name><surname>Dandan</surname><given-names>Z.</given-names></name><name><surname>Applications</surname><given-names>C.S.J.C.E.a.</given-names></name></person-group><series>Research on Vehicle Detection and Tracking Algorithms in Traffic Monitoring Scenarios</series><volume>vol. 57</volume><year>2021</year><fpage>103</fpage><lpage>111</lpage></element-citation></ref><ref id="bib31"><label>32</label><element-citation publication-type="journal" id="sref31"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Anguelov</surname><given-names>D.</given-names></name><name><surname>Erhan</surname><given-names>D.</given-names></name><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Reed</surname><given-names>S.</given-names></name><name><surname>Fu</surname><given-names>C.-Y.</given-names></name><name><surname>Berg</surname><given-names>A.C.</given-names></name></person-group><article-title>Ssd: single shot multibox detector</article-title><source>Proceedings of the Computer Vision&#x02013;ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11&#x02013;14, 2016, Proceedings, Part I</source><volume>14</volume><year>2016</year><fpage>21</fpage><lpage>37</lpage></element-citation></ref><ref id="bib30"><label>33</label><element-citation publication-type="book" id="sref30"><person-group person-group-type="author"><name><surname>Caihong</surname><given-names>L.</given-names></name><name><surname>Lei</surname><given-names>Z.</given-names></name><name><surname>Technology</surname><given-names>H.H.J.J.o.C.S.a.</given-names></name></person-group><series>Visual Tracking of Cross-View Multi-Object in Traffic Intersection Surveillance Videos</series><volume>vol. 41</volume><year>2018</year><fpage>221</fpage><lpage>235</lpage></element-citation></ref><ref id="bib33"><label>34</label><element-citation publication-type="book" id="sref33"><person-group person-group-type="author"><name><surname>Du</surname><given-names>Y.</given-names></name><name><surname>Wan</surname><given-names>J.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Tong</surname><given-names>Z.</given-names></name><name><surname>Dong</surname><given-names>J. Giaotracker</given-names></name></person-group><part-title>A comprehensive framework for mcmot with global information and optimizing strategies in visdrone 2021</part-title><source>Proceedings of the Proceedings of the IEEE/CVF International Conference on Computer Vision</source><year>2021</year><fpage>2809</fpage><lpage>2819</lpage></element-citation></ref></ref-list></back></article>