from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.prompts import ChatPromptTemplate
# å¯¼å…¥Ollamaçš„å¤§è¯­è¨€æ¨¡å‹ç±»
from langchain_community.llms import Ollama
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# --- 1. é…ç½®å‚æ•° ---
CHROMA_PERSIST_DIR = "./chroma_db_ollama"  # ç¡®ä¿å­˜å‚¨å’ŒåŠ è½½çš„æ˜¯åŒä¸€ä¸ªè·¯å¾„
OLLAMA_EMBEDDING_MODEL = "embeddinggemma:latest"
# âœ… ä¿®æ”¹ï¼šæŒ‡å®šè¦ä½¿ç”¨çš„Ollamaå¤§è¯­è¨€æ¨¡å‹
OLLAMA_LLM_MODEL = "deepseek-r1:8b"  # ä½ ä¹Ÿå¯ä»¥æ¢æˆ "qwen:7b" ç­‰


def main():
    print(f"--- åŒ»å­¦æ–‡çŒ®æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (æœ¬åœ°æ¨¡å‹: {OLLAMA_LLM_MODEL}) ---")

    # --- 2. åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“ ---
    print("æ­£åœ¨åŠ è½½æœ¬åœ°çŸ¥è¯†åº“...")
    try:
        embedding_function = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)
        vector_db = Chroma(
            persist_directory=CHROMA_PERSIST_DIR,
            embedding_function=embedding_function
        )
    except Exception as e:
        print(f"é”™è¯¯ï¼šæ— æ³•åŠ è½½å‘é‡æ•°æ®åº“ã€‚è¯·å…ˆè¿è¡Œ 'build_index.py' (Ollamaç‰ˆæœ¬)ã€‚ {e}")
        return

    # --- 3. å‡†å¤‡RAGçš„æ ¸å¿ƒç»„ä»¶ ---
    retriever = vector_db.as_retriever(search_kwargs={"k": 5})

    # Promptæ¨¡æ¿å¯ä»¥ä¿æŒä¸å˜ï¼Œä½†å¯ä»¥æ ¹æ®æ¨¡å‹çš„ç‰¹ç‚¹å¾®è°ƒ
    template = """
    You are a professional medical research assistant. Use the following pieces of context to answer the question at the end. 
    If you don't know the answer from the context, just say that you don't know, don't try to make up an answer.
    Answer in Chinese.

    Context:
    {context}

    Question:
    {question}

    Answer (in Chinese):
    """
    prompt = ChatPromptTemplate.from_template(template)

    # âœ… ä¿®æ”¹ï¼šåŠ è½½Ollamaå¤§è¯­è¨€æ¨¡å‹
    llm = Ollama(model=OLLAMA_LLM_MODEL)

    # --- 4. æ„å»ºRAGå¤„ç†é“¾ (æ­¤éƒ¨åˆ†ä¸å˜) ---
    rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    print("çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œå¯ä»¥å¼€å§‹æé—®äº† (è¾“å…¥ 'é€€å‡º' æ¥ç»“æŸç¨‹åº)ã€‚\n")

    # --- 5. åˆ›å»ºä¸€ä¸ªäº¤äº’å¼æŸ¥è¯¢å¾ªç¯ (æ­¤éƒ¨åˆ†ä¸å˜) ---
    while True:
        user_question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
        if user_question.lower() == 'é€€å‡º':
            break

        print("\næ­£åœ¨æ£€ç´¢å¹¶ç”Ÿæˆç­”æ¡ˆ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")

        # è°ƒç”¨RAGé“¾æ¥è·å–ç­”æ¡ˆ
        answer = rag_chain.invoke(user_question)

        print("\nğŸ’¡ ç­”æ¡ˆ:")
        print(answer)
        print("-" * 50)

        relevant_docs = retriever.get_relevant_documents(user_question)
        print("ğŸ” å‚è€ƒæ–‡çŒ®ç‰‡æ®µ:")
        for i, doc in enumerate(relevant_docs):
            print(f"  ç‰‡æ®µ {i + 1} (æ¥è‡ªPMID: {doc.metadata.get('pmid', 'N/A')}):")
            print(f"  > {doc.page_content[:200]}...\n")

    print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")


if __name__ == "__main__":
    main()