import os
import uuid
import boto3
import json
from botocore.exceptions import ClientError
from dotenv import load_dotenv
import chromadb
from chromadb.utils import embedding_functions
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict, Optional

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class MedicalRAGPipeline:
    def __init__(self):
        """åˆå§‹åŒ–RAGæµæ°´çº¿ç»„ä»¶"""
        # åˆå§‹åŒ–S3å®¢æˆ·ç«¯
        with open("../config.json") as f:
            cfg = json.load(f)

        self.s3_client = boto3.client(
            cfg["service"],
            aws_access_key_id=cfg["aws_access_key_id"],
            aws_secret_access_key=cfg["aws_secret_access_key"],
            region_name=cfg["region_name"]
        )
        
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰
        self.chroma_client = chromadb.PersistentClient(path="./medical_chroma_db")
        
        # åˆå§‹åŒ–åŒ»å­¦é¢†åŸŸåµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨BioBERTè¡ç”Ÿæ¨¡å‹ï¼‰
        self.embedding_func = embedding_functions.HuggingFaceEmbeddingFunction(
            model_name="pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨ï¼ˆé’ˆå¯¹åŒ»å­¦æ–‡æ¡£ä¼˜åŒ–ï¼‰
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,           # æ¯ä¸ªç‰‡æ®µçš„å­—ç¬¦æ•°
            chunk_overlap=50,         # ç‰‡æ®µé‡å éƒ¨åˆ†ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
            separators=["\n\n", "\n", ". ", " ", ""],  # ä¼˜å…ˆæŒ‰æ®µè½å’Œå¥å­åˆ‡åˆ†
            length_function=len
        )
        
        # è·å–æˆ–åˆ›å»ºChromaé›†åˆ
        self.collection = self.chroma_client.get_or_create_collection(
            name="medical_articles",
            embedding_function=self.embedding_func,
            metadata={"description": "å­˜å‚¨åŒ»å­¦æ–‡ç« ç‰‡æ®µåŠå…¶å‘é‡"}
        )

    def load_document_from_s3(self, bucket_name: str, s3_key: str) -> Optional[Dict]:
        """ä»S3åŠ è½½æ–‡æ¡£å†…å®¹"""
        try:
            response = self.s3_client.get_object(Bucket=bucket_name, Key=s3_key)
            content = response['Body'].read().decode('utf-8', errors='ignore')
            
            # æå–åŸºæœ¬å…ƒæ•°æ®
            metadata = {
                "s3_bucket": bucket_name,
                "s3_key": s3_key,
                "s3_path": f"s3://{bucket_name}/{s3_key}",
                "file_name": os.path.basename(s3_key),
                "file_size": len(content)
            }
            
            return {
                "content": content,
                "metadata": metadata
            }
        except ClientError as e:
            print(f"âŒ ä»S3åŠ è½½æ–‡æ¡£å¤±è´¥: {str(e)}")
            return None
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            return None

    def split_document(self, content: str) -> List[str]:
        """å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºé€‚åˆRAGçš„ç‰‡æ®µ"""
        return self.text_splitter.split_text(content)

    def process_and_store_document(self, bucket_name: str, s3_key: str, category: str) -> bool:
        """å¤„ç†æ–‡æ¡£å¹¶å­˜å‚¨åˆ°ChromaDB"""
        # 1. ä»S3åŠ è½½æ–‡æ¡£
        doc_data = self.load_document_from_s3(bucket_name, s3_key)
        if not doc_data:
            return False
        
        # 2. åˆ‡åˆ†æ–‡æ¡£
        chunks = self.split_document(doc_data["content"])
        if not chunks:
            print("âŒ æ–‡æ¡£åˆ‡åˆ†åæ²¡æœ‰å†…å®¹")
            return False
        
        print(f"âœ… æ–‡æ¡£åˆ‡åˆ†å®Œæˆ: {len(chunks)}ä¸ªç‰‡æ®µ")
        
        # 3. ä¸ºæ¯ä¸ªç‰‡æ®µå‡†å¤‡æ•°æ®
        ids = []
        documents = []
        metadatas = []
        
        for i, chunk in enumerate(chunks):
            # ç”Ÿæˆå”¯ä¸€ID
            chunk_id = f"{uuid.uuid4()}"
            
            # æ„å»ºå…ƒæ•°æ®ï¼ˆåŒ…å«S3è·¯å¾„å’Œåˆ†ç±»ä¿¡æ¯ï¼‰
            chunk_metadata = {
                **doc_data["metadata"],  # ç»§æ‰¿æ–‡æ¡£çº§å…ƒæ•°æ®
                "chunk_id": i,
                "category": category,    # åŒ»å­¦åˆ†ç±»ï¼ˆå¦‚"lung_cancer"ï¼‰
                "chunk_length": len(chunk)
            }
            
            ids.append(chunk_id)
            documents.append(chunk)
            metadatas.append(chunk_metadata)
        
        # 4. å­˜å‚¨åˆ°ChromaDB
        try:
            self.collection.add(
                ids=ids,
                documents=documents,
                metadatas=metadatas
                # åµŒå…¥å‘é‡ä¼šè‡ªåŠ¨é€šè¿‡embedding_functionç”Ÿæˆ
            )
            print(f"âœ… æˆåŠŸå­˜å‚¨{len(chunks)}ä¸ªç‰‡æ®µåˆ°ChromaDB")
            return True
        except Exception as e:
            print(f"âŒ å­˜å‚¨åˆ°ChromaDBå¤±è´¥: {str(e)}")
            return False

    def batch_process_documents(self, bucket_name: str, s3_prefix: str, category: str) -> Dict:
        """æ‰¹é‡å¤„ç†S3æŒ‡å®šå‰ç¼€ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        stats = {
            "total": 0,
            "success": 0,
            "failed": 0
        }
        
        try:
            # åˆ—å‡ºS3å‰ç¼€ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
            response = self.s3_client.list_objects_v2(Bucket=bucket_name, Prefix=s3_prefix)
            if 'Contents' not in response:
                print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: s3://{bucket_name}/{s3_prefix}")
                return stats
            
            # æ‰¹é‡å¤„ç†æ¯ä¸ªæ–‡ä»¶
            for obj in response['Contents']:
                s3_key = obj['Key']
                # è·³è¿‡ç›®å½•ï¼ˆä»…å¤„ç†æ–‡ä»¶ï¼‰
                if s3_key.endswith('/'):
                    continue
                
                stats["total"] += 1
                print(f"\nå¤„ç†æ–‡ä»¶ {stats['total']}/{len(response['Contents'])}: {s3_key}")
                
                if self.process_and_store_document(bucket_name, s3_key, category):
                    stats["success"] += 1
                else:
                    stats["failed"] += 1
            
            print(f"\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ: å…±{stats['total']}ä¸ª, æˆåŠŸ{stats['success']}ä¸ª, å¤±è´¥{stats['failed']}ä¸ª")
            return stats
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å‡ºé”™: {str(e)}")
            return stats

    def retrieve_relevant_chunks(self, query: str, n_results: int = 5, category: Optional[str] = None) -> List[Dict]:
        """æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ"""
        # æ„å»ºè¿‡æ»¤æ¡ä»¶ï¼ˆå¯é€‰ï¼‰
        where_clause = {"category": category} if category else None
        
        # æ‰§è¡Œæ£€ç´¢
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results,
            where=where_clause
        )
        
        # æ•´ç†ç»“æœ
        retrieved = []
        for i in range(len(results['ids'][0])):
            retrieved.append({
                "id": results['ids'][0][i],
                "content": results['documents'][0][i],
                "metadata": results['metadatas'][0][i],
                "similarity_score": 1 - results['distances'][0][i]  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
            })
        
        return retrieved

    def get_document_from_s3(self, s3_path: str) -> Optional[str]:
        """ä»S3è·å–å®Œæ•´æ–‡æ¡£å†…å®¹"""
        try:
            # è§£æS3è·¯å¾„
            bucket = s3_path.split("//")[1].split("/")[0]
            key = "/".join(s3_path.split("//")[1].split("/")[1:])
            
            response = self.s3_client.get_object(Bucket=bucket, Key=key)
            return response['Body'].read().decode('utf-8', errors='ignore')
        except Exception as e:
            print(f"âŒ è·å–å®Œæ•´æ–‡æ¡£å¤±è´¥: {str(e)}")
            return None

if __name__ == "__main__":
    # åˆå§‹åŒ–RAGæµæ°´çº¿
    rag_pipeline = MedicalRAGPipeline()
    
    # é…ç½®å‚æ•°ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å®é™…ä¿¡æ¯ï¼‰
    BUCKET_NAME = "id2221"
    S3_PREFIX = "10_01/domain_partitioned/"  # S3ä¸­æ–‡æ¡£æ‰€åœ¨è·¯å¾„
    DOCUMENT_CATEGORY = "lung_cancer"  # æ–‡æ¡£åˆ†ç±»
    
    # 1. æ‰¹é‡å¤„ç†æ–‡æ¡£å¹¶å­˜å‚¨åˆ°ChromaDB
    print("===== å¼€å§‹æ‰¹é‡å¤„ç†æ–‡æ¡£ =====")
    rag_pipeline.batch_process_documents(BUCKET_NAME, S3_PREFIX, DOCUMENT_CATEGORY)
    
    # 2. æµ‹è¯•æ£€ç´¢åŠŸèƒ½
    print("\n===== æµ‹è¯•æ£€ç´¢åŠŸèƒ½ =====")
    test_queries = [
        "è‚ºç™Œçš„é¶å‘æ²»ç–—æœ€æ–°è¿›å±•",
        "éå°ç»†èƒè‚ºç™Œçš„å…ç–«æ²»ç–—æ–¹æ³•",
        "å¥¥å¸Œæ›¿å°¼åœ¨è‚ºç™Œæ²»ç–—ä¸­çš„åº”ç”¨"
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        print("------------------------")
        results = rag_pipeline.retrieve_relevant_chunks(
            query=query,
            n_results=3,
            category=DOCUMENT_CATEGORY
        )
        
        for i, result in enumerate(results, 1):
            print(f"\nç»“æœ {i} (ç›¸ä¼¼åº¦: {result['similarity_score']:.4f}):")
            print(f"æ¥æº: {result['metadata']['s3_path']}")
            print(f"ç‰‡æ®µ: {result['content'][:200]}...")
